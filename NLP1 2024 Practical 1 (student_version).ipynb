{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-aRiOgl4nHg"
      },
      "source": [
        "------\n",
        "**You cannot save any changes you make to this file, so please make sure to save it on your Google Colab drive or download it as a .ipynb file.**\n",
        "\n",
        "------\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIZrAUx57vsM"
      },
      "source": [
        "Practical 1: Sentiment Detection in Movie Reviews\n",
        "========================================\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4kXPMhyngZW"
      },
      "source": [
        "This practical concerns detecting sentiment in movie reviews. This is a typical NLP classification task.\n",
        "In [this file](https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json) (80MB) you will find 1000 positive and 1000 negative **movie reviews**.\n",
        "Each review is a **document** and consists of one or more sentences.\n",
        "\n",
        "To prepare yourself for this practical, you should\n",
        "have a look at a few of these texts to understand the difficulties of\n",
        "the task: how might one go about classifying the texts? You will write\n",
        "code that decides whether a movie review conveys positive or\n",
        "negative sentiment.\n",
        "\n",
        "Please make sure you have read the following paper:\n",
        "\n",
        ">   Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan\n",
        "(2002).\n",
        "[Thumbs up? Sentiment Classification using Machine Learning\n",
        "Techniques](https://dl.acm.org/citation.cfm?id=1118704). EMNLP.\n",
        "\n",
        "Bo Pang et al. introduced the movie review sentiment\n",
        "classification task, and the above paper was one of the first papers on\n",
        "the topic. The first version of your sentiment classifier will do\n",
        "something similar to Pang et al.'s system. If you have questions about it,\n",
        "you should resolve you doubts as soon as possible with your TA.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb7errgRASzZ"
      },
      "source": [
        "**Advice**\n",
        "\n",
        "Please read through the entire practical and familiarise\n",
        "yourself with all requirements before you start coding or otherwise\n",
        "solving the tasks. Writing clean and concise code can make the difference\n",
        "between solving the assignment in a matter of hours, and taking days to\n",
        "run all experiments.\n",
        "\n",
        "\n",
        "**Implementation**\n",
        "\n",
        "While we inserted code cells to indicate where you should implement your own code, please feel free to add/remove code blocks where you see fit (but make sure that the general structure of the assignment is preserved). Also, please keep in mind that it is always good practice to structure your code properly, e.g., by implementing separate classes and functions that can be reused. **Make sure you run all your code before submitting the notebook, and do not leave unnecessary print statements / cells in your notebook that are not intended for the grader.**\n",
        "\n",
        "## Environment\n",
        "\n",
        "All code should be written in **Python 3**.\n",
        "This is the default in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaZnxptMJiD7",
        "outputId": "a101494a-7801-4c8a-eacc-c2509bdbf2ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.9.9\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYZyIF7lJnGn"
      },
      "source": [
        "If you want to run code on your own computer, then download this notebook through `File -> Download .ipynb`.\n",
        "The easiest way to\n",
        "install Python is through downloading\n",
        "[Anaconda](https://www.anaconda.com/download).\n",
        "After installation, you can start the notebook by typing `jupyter notebook filename.ipynb`.\n",
        "You can also use an IDE\n",
        "such as [PyCharm](https://www.jetbrains.com/pycharm/download/) to make\n",
        "coding and debugging easier. It is good practice to create a [virtual\n",
        "environment](https://docs.python.org/3/tutorial/venv.html) for this\n",
        "project, so that any Python packages don’t interfere with other\n",
        "projects.\n",
        "\n",
        "\n",
        "**Learning Python 3**\n",
        "\n",
        "If you are new to Python 3, you may want to check out a few of these resources:\n",
        "- https://learnxinyminutes.com/docs/python3/\n",
        "- https://www.learnpython.org/\n",
        "- https://docs.python.org/3/tutorial/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hok-BFu9lGoK",
        "outputId": "b2dd9e94-8388-407c-851f-eb3a63e2cc52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\lucyn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
            "c:\\Users\\lucyn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
            "c:\\Users\\lucyn\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
            "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
            "C:\\Users\\lucyn\\AppData\\Roaming\\Python\\Python39\\site-packages\\matplotlib\\projections\\__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
            "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "from subprocess import call\n",
        "from nltk import FreqDist\n",
        "from nltk.util import ngrams\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import sklearn as sk\n",
        "# from google.colab import drive\n",
        "import pickle\n",
        "import json\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXWyGHwE-ieQ"
      },
      "source": [
        "## Loading the data\n",
        "\n",
        "**Download the sentiment lexicon and the movie reviews dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm-rakqtlMOT",
        "outputId": "9ab901e9-e979-44d9-b9d8-9b2057593f8e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n",
            "'wget' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "# download sentiment lexicon\n",
        "!wget https://gist.githubusercontent.com/bastings/d6f99dcb6c82231b94b013031356ba05/raw/f80a0281eba8621b122012c89c8b5e2200b39fd6/sent_lexicon\n",
        "# download review data\n",
        "!wget https://gist.githubusercontent.com/bastings/d47423301cca214e3930061a5a75e177/raw/5113687382919e22b1f09ce71a8fecd1687a5760/reviews.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkPwuHp5LSuQ"
      },
      "source": [
        "**Load the movie reviews.**\n",
        "\n",
        "Each word in a review comes with its part-of-speech tag. For documentation on POS-tags, see https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "careEKj-mRpl",
        "outputId": "b7db8b44-21f6-4e47-e4ac-1623c74ef42d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of reviews: 2000 \n",
            "\n",
            "0 NEG 29\n",
            "Two/CD teen/JJ couples/NNS go/VBP to/TO a/DT church/NN party/NN ,/, drink/NN and/CC then/RB drive/NN ./.\n",
            "1 NEG 11\n",
            "Damn/JJ that/IN Y2K/CD bug/NN ./.\n",
            "2 NEG 24\n",
            "It/PRP is/VBZ movies/NNS like/IN these/DT that/WDT make/VBP a/DT jaded/JJ movie/NN viewer/NN thankful/JJ for/IN the/DT invention/NN of/IN the/DT Timex/NNP IndiGlo/NNP watch/NN ./.\n",
            "3 NEG 19\n",
            "QUEST/NN FOR/IN CAMELOT/NNP ``/`` Quest/NNP for/IN Camelot/NNP ''/'' is/VBZ Warner/NNP Bros./NNP '/POS first/JJ feature-length/JJ ,/, fully-animated/JJ attempt/NN to/TO steal/VB clout/NN from/IN Disney/NNP 's/POS cartoon/NN empire/NN ,/, but/CC the/DT mouse/NN has/VBZ no/DT reason/NN to/TO be/VB worried/VBN ./.\n",
            "4 NEG 38\n",
            "Synopsis/NNPS :/: A/DT mentally/RB unstable/JJ man/NN undergoing/VBG psychotherapy/NN saves/VBZ a/DT boy/NN from/IN a/DT potentially/RB fatal/JJ accident/NN and/CC then/RB falls/VBZ in/IN love/NN with/IN the/DT boy/NN 's/POS mother/NN ,/, a/DT fledgling/NN restauranteur/NN ./.\n",
            "\n",
            "Number of word types: 47743\n",
            "Number of word tokens: 1512359\n",
            "\n",
            "Most common tokens:\n",
            "         , :    77842\n",
            "       the :    75948\n",
            "         . :    59027\n",
            "         a :    37583\n",
            "       and :    35235\n",
            "        of :    33864\n",
            "        to :    31601\n",
            "        is :    25972\n",
            "        in :    21563\n",
            "        's :    18043\n",
            "        it :    15904\n",
            "      that :    15820\n",
            "     -rrb- :    11768\n",
            "     -lrb- :    11670\n",
            "        as :    11312\n",
            "      with :    10739\n",
            "       for :     9816\n",
            "       his :     9542\n",
            "      this :     9497\n",
            "      film :     9404\n"
          ]
        }
      ],
      "source": [
        "# file structure:\n",
        "# [\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
        "#  {\"cv\": integer, \"sentiment\": str, \"content\": list}\n",
        "#   ..\n",
        "# ]\n",
        "# where `content` is a list of sentences,\n",
        "# with a sentence being a list of (token, pos_tag) pairs.\n",
        "\n",
        "\n",
        "with open(\"reviews.json\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  reviews = json.load(f)\n",
        "\n",
        "print(\"Total number of reviews:\", len(reviews), '\\n')\n",
        "\n",
        "def print_sentence_with_pos(s):\n",
        "  print(\" \".join(\"%s/%s\" % (token, pos_tag) for token, pos_tag in s))\n",
        "\n",
        "for i, r in enumerate(reviews):\n",
        "  print(r[\"cv\"], r[\"sentiment\"], len(r[\"content\"]))  # cv, sentiment, num sents\n",
        "  print_sentence_with_pos(r[\"content\"][0])\n",
        "  if i == 4:\n",
        "    break\n",
        "\n",
        "c = Counter()\n",
        "for review in reviews:\n",
        "  for sentence in review[\"content\"]:\n",
        "    for token, pos_tag in sentence:\n",
        "      c[token.lower()] += 1\n",
        "\n",
        "print(\"\\nNumber of word types:\", len(c))\n",
        "print(\"Number of word tokens:\", sum(c.values()))\n",
        "\n",
        "print(\"\\nMost common tokens:\")\n",
        "for token, count in c.most_common(20):\n",
        "  print(\"%10s : %8d\" % (token, count))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6PWaEoh8B34"
      },
      "source": [
        "#(1) Lexicon-based approach (3.5pts)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsTSMb6ma4E8"
      },
      "source": [
        "A traditional approach to classify documents according to their sentiment is the lexicon-based approach. To implement this approach, you need a **sentiment lexicon**, i.e., a list of words annotated with a sentiment label (e.g., positive and negative, or a score from 0 to 5).\n",
        "\n",
        "In this practical, you will use the sentiment\n",
        "lexicon released by Wilson et al. (2005).\n",
        "\n",
        "> Theresa Wilson, Janyce Wiebe, and Paul Hoffmann\n",
        "(2005). [Recognizing Contextual Polarity in Phrase-Level Sentiment\n",
        "Analysis](http://www.aclweb.org/anthology/H/H05/H05-1044.pdf). HLT-EMNLP.\n",
        "\n",
        "Pay attention to all the information available in the sentiment lexicon. The field *word1* contains the lemma, *priorpolarity* contains the sentiment label (positive, negative, both, or neutral), *type* gives you the magnitude of the word's sentiment (strong or weak), and *pos1* gives you the part-of-speech tag of the lemma. Some lemmas can have multiple part-of-speech tags and thus multiple entries in the lexicon. The path of the lexicon file is `\"sent_lexicon\"`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ogq0Eq2hQglh",
        "outputId": "e2a5c7f0-8f4f-4606-bdb7-5a1895511907"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
            "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
            "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n"
          ]
        }
      ],
      "source": [
        "with open(\"sent_lexicon\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "  line_cnt = 0\n",
        "  for line in f:\n",
        "    print(line.strip())\n",
        "    line_cnt += 1\n",
        "    if line_cnt > 4:\n",
        "      break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mml4nOtIUBhn"
      },
      "source": [
        "Lexica such as this can be used to solve\n",
        "the classification task without using Machine Learning. For example, one might look up every word $w_1 ... w_n$ in a document, and compute a **binary score**\n",
        "$S_{binary}$ by counting how many words have a positive or a\n",
        "negative label in the sentiment lexicon $SLex$.\n",
        "\n",
        "$$S_{binary}(w_1 w_2 ... w_n) = \\sum_{i = 1}^{n}\\text{sign}(SLex\\big[w_i\\big])$$\n",
        "\n",
        "where $\\text{sign}(SLex\\big[w_i\\big])$ refers to the polarity of $w_i$.\n",
        "\n",
        "**Threshold.** On average, there are more positive than negative words per review (~7.13 more positive than negative per review) to take this bias into account you should use a threshold of **8** (roughly the bias itself) to make it harder to classify as positive.\n",
        "\n",
        "$$\n",
        "\\text{classify}(S_{binary}(w_1 w_2 ... w_n)) = \\bigg\\{\\begin{array}{ll}\n",
        "        \\text{positive} & \\text{if } S_{binary}(w_1w_2...w_n) > threshold\\\\\n",
        "        \\text{negative} & \\text{otherwise}\n",
        "        \\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOFnMvbeeZrc"
      },
      "source": [
        "#### (Q1.1) Implement this approach and report its classification accuracy. (1 pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ED2aTEYutW1-"
      },
      "outputs": [],
      "source": [
        "def parse_lexicon(file_path):\n",
        "    lexicon = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            attributes = line.strip().split()\n",
        "            word = None\n",
        "            polarity = None\n",
        "\n",
        "            for attr in attributes:\n",
        "                key, value = attr.split('=')\n",
        "                if key == \"word1\":\n",
        "                    word = value\n",
        "                elif key == \"priorpolarity\":\n",
        "                    polarity = value\n",
        "\n",
        "            if polarity == \"positive\":\n",
        "                lexicon[word] = 1\n",
        "            elif polarity == \"negative\":\n",
        "                lexicon[word] = -1\n",
        "            elif polarity == \"neutral\":\n",
        "                lexicon[word] = 0\n",
        "\n",
        "    return lexicon\n",
        "\n",
        "def compute_review_scores(json_file_path, lexicon):\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        reviews = json.load(f)\n",
        "\n",
        "    review_scores = []\n",
        "    real_scores = []\n",
        "\n",
        "    for review in reviews:\n",
        "        score = 0\n",
        "        for sentence in review[\"content\"]:\n",
        "            for word, tag in sentence:\n",
        "                word = word.lower()\n",
        "                if word in lexicon:\n",
        "                    score += lexicon[word]\n",
        "\n",
        "        if score > 8:\n",
        "            binary_score = 1\n",
        "        else:\n",
        "            binary_score = -1\n",
        "\n",
        "        review_scores.append(binary_score)\n",
        "        real_scores.append(1 if review[\"sentiment\"] == \"POS\" else -1)\n",
        "\n",
        "    return review_scores, real_scores\n",
        "\n",
        "# Example usage:\n",
        "lexicon = parse_lexicon(\"sent_lexicon\")\n",
        "review_scores, real_scores = compute_review_scores(\"reviews.json\", lexicon)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy528EUTphz5",
        "outputId": "0ff97ef4-9284-40fb-8d6a-5a49961f2fce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6775\n"
          ]
        }
      ],
      "source": [
        "# token_results should be a list of binary indicators; for example [1, 0, 1, ...]\n",
        "# where 1 indicates a correct classification and 0 an incorrect classification.\n",
        "\n",
        "token_results = []\n",
        "for i in range(len(review_scores)):\n",
        "    if review_scores[i] == real_scores[i]:\n",
        "        token_results.append(1)\n",
        "    else:\n",
        "        token_results.append(0)\n",
        "# print(token_results)\n",
        "\n",
        "token_accuracy = np.sum(token_results)/len(review_scores)\n",
        "\n",
        "print(\"Accuracy:\", token_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twox0s_3eS0V"
      },
      "source": [
        "As the sentiment lexicon also has information about the **magnitude** of\n",
        "sentiment (e.g., *“excellent\"* has the same sentiment _polarity_ as *“good\"* but it has a higher magnitude), we can take a more fine-grained approach by adding up all\n",
        "sentiment scores, and deciding the polarity of the movie review using\n",
        "the sign of the weighted score $S_{weighted}$.\n",
        "\n",
        "$$S_{weighted}(w_1w_2...w_n) = \\sum_{i = 1}^{n}SLex\\big[w_i\\big]$$\n",
        "\n",
        "\n",
        "Make sure you define an appropriate threshold for this approach.\n",
        "\n",
        "#### (Q1.2) Now incorporate magnitude information and report the classification accuracy. Don't forget to use the threshold. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG3hUDnPtkhS"
      },
      "outputs": [],
      "source": [
        "def parse_lexicon(file_path):\n",
        "    lexicon = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            attributes = line.strip().split()\n",
        "            word = None\n",
        "            polarity = None\n",
        "            magnitude = None\n",
        "\n",
        "            for attr in attributes:\n",
        "                key, value = attr.split('=')\n",
        "                if key == \"word1\":\n",
        "                    word = value\n",
        "                elif key == \"priorpolarity\":\n",
        "                    polarity = value\n",
        "                elif key == \"type\":\n",
        "                    magnitude = value\n",
        "\n",
        "            if polarity == \"positive\" and magnitude == \"strongsubj\":\n",
        "                lexicon[word] = 3 #chosen arbitrarily\n",
        "            elif polarity == \"positive\" and magnitude == \"weaksubj\":\n",
        "                lexicon[word] = 1\n",
        "            elif polarity == \"neutral\":\n",
        "                lexicon[word] = 0\n",
        "            elif polarity == \"negative\" and magnitude == \"strongsubj\":\n",
        "                lexicon[word] = -3 #chosen arbitrarily\n",
        "            elif polarity == \"negative\" and magnitude == \"weaksubj\":\n",
        "                lexicon[word] = -1\n",
        "\n",
        "    return lexicon\n",
        "\n",
        "def compute_review_scores(json_file_path, lexicon):\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        reviews = json.load(f)\n",
        "\n",
        "    review_scores = []\n",
        "    real_scores = []\n",
        "    mean_score = 0\n",
        "    for review in reviews:\n",
        "        score = 0\n",
        "        for sentence in review[\"content\"]:\n",
        "            for word, _ in sentence:\n",
        "                word = word.lower()\n",
        "                if word in lexicon:\n",
        "                    score += lexicon[word]\n",
        "\n",
        "        if score > 8:\n",
        "            binary_score = 1\n",
        "        elif score <= 8:\n",
        "            binary_score = -1\n",
        "        mean_score += score\n",
        "\n",
        "        review_scores.append(binary_score)\n",
        "        real_scores.append(1 if review[\"sentiment\"] == \"POS\" else -1)\n",
        "\n",
        "    # print((mean_score)/len(reviews))\n",
        "\n",
        "    return review_scores, real_scores\n",
        "\n",
        "\n",
        "lexicon = parse_lexicon(\"sent_lexicon\")\n",
        "review_scores, real_scores = compute_review_scores(\"reviews.json\", lexicon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vVk7CvDpyka",
        "outputId": "5aa00315-1137-4b14-f9bc-f3960331bf4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6925\n"
          ]
        }
      ],
      "source": [
        "magnitude_results = []\n",
        "for i in range(len(review_scores)):\n",
        "    if review_scores[i] == real_scores[i]:\n",
        "        magnitude_results.append(1)\n",
        "    else:\n",
        "        magnitude_results.append(0)\n",
        "# print(magnitude_results)\n",
        "\n",
        "magnitude_accuracy = np.sum(magnitude_results)/len(magnitude_results)\n",
        "print(\"Accuracy:\" , magnitude_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9SHoGPfsAHV"
      },
      "source": [
        "#### (Q.1.3) Make a barplot of the two results (0.5pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "8LgBcYcXsEk3",
        "outputId": "f6bf5cf8-bbfa-4f97-9ee6-86a19bfbbd62"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "labels = ['Normal Magnitude', 'Adjusted Magnitude']\n",
        "values = [token_accuracy, magnitude_accuracy]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(labels, values, color = ['teal', 'orange'])\n",
        "plt.xlabel('Approaches')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Change with Magnitude')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNhS8OCVxMHd"
      },
      "source": [
        "#### (Q1.4) A better threshold (1pt)\n",
        "Above we have defined a threshold to account for an inherent bias in the dataset: there are more positive than negative words per review.\n",
        "However, that threshold does not take into account *document length*. Explain why this is a problem and implement an alternative way to compute the threshold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo7gk1I-omLI"
      },
      "source": [
        "> This might be a problem as just because a review is longer doesn't necessarily mean the sentiment is stronger. For example a longer neutral review could naturally have more both positive and negative words while a short negative review would have less, even when their sentiments are otherwise clear. Thus, we should normalize by the length of the review. As below, we implemented a word count per review counter by which to normalize. Here it was important to set the threshold correctly so we calculated it by taking the avarage score as the dataset. Overall this approach should ensure that the length of the review and thus incrased pos/neg word counts don't disproportionaltely influence the sentiment classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dwt0B8h8aKjr",
        "outputId": "44ddc26d-24ae-4f0f-8d6d-e9fafb04d2ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.01841402592400109\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "def parse_lexicon(file_path):\n",
        "    lexicon = {}\n",
        "    with open(file_path, 'r') as f:\n",
        "\n",
        "        for line in f:\n",
        "\n",
        "            attributes = line.strip().split()\n",
        "            word = None\n",
        "            polarity = None\n",
        "            magnitude = None\n",
        "\n",
        "            for attr in attributes:\n",
        "                key, value = attr.split('=')\n",
        "                if key == \"word1\":\n",
        "                    word = value\n",
        "                elif key == \"priorpolarity\":\n",
        "                    polarity = value\n",
        "                elif key == \"type\":\n",
        "                    magnitude = value\n",
        "\n",
        "            if polarity == \"positive\" and magnitude == \"strongsubj\":\n",
        "                lexicon[word] = 3 #chosen arbitrarily\n",
        "            elif polarity == \"positive\" and magnitude == \"weaksubj\":\n",
        "                lexicon[word] = 1\n",
        "            elif polarity == \"neutral\":\n",
        "                lexicon[word] = 0\n",
        "            elif polarity == \"negative\" and magnitude == \"strongsubj\":\n",
        "                lexicon[word] = -3 #chosen arbitrarily\n",
        "            elif polarity == \"negative\" and magnitude == \"weaksubj\":\n",
        "                lexicon[word] = -1\n",
        "\n",
        "    return lexicon\n",
        "\n",
        "def compute_review_scores(json_file_path, lexicon):\n",
        "    with open(json_file_path, 'r') as f:\n",
        "        reviews = json.load(f)\n",
        "\n",
        "    review_scores = []\n",
        "    real_scores = []\n",
        "    mean_score = 0\n",
        "    for review in reviews:\n",
        "        score = 0\n",
        "        sentence_count  = 0\n",
        "        word_count = 0\n",
        "        for sentence in review[\"content\"]:\n",
        "            sentence_count += 1\n",
        "            for word, _ in sentence:\n",
        "                word = word.lower()\n",
        "                word_count += 1\n",
        "                if word in lexicon:\n",
        "                    score += lexicon[word]\n",
        "\n",
        "        if word_count > 0:\n",
        "            norm_score = score / word_count\n",
        "\n",
        "        else: # if somehow there are no words in the review prevent division by zero\n",
        "            norm_score = score\n",
        "\n",
        "        if norm_score > 0.018:\n",
        "            binary_score = 1\n",
        "        else:\n",
        "            binary_score = -1\n",
        "        mean_score += norm_score\n",
        "        review_scores.append(binary_score)\n",
        "        real_scores.append(1 if review[\"sentiment\"] == \"POS\" else -1)\n",
        "\n",
        "    print(mean_score/len(reviews)) # this is how i get my threshold\n",
        "\n",
        "    return review_scores, real_scores\n",
        "\n",
        "lexicon = parse_lexicon(\"sent_lexicon\")\n",
        "\n",
        "review_scores, real_scores = compute_review_scores(\"reviews.json\", lexicon)\n",
        "\n",
        "accuracy = 0\n",
        "for i in range(len(review_scores)):\n",
        "    if review_scores[i] == real_scores[i]:\n",
        "        accuracy += 1\n",
        "accuracy = accuracy / len(review_scores)\n",
        "\n",
        "\n",
        "# print(review_scores)\n",
        "# print(real_scores)\n",
        "# print(accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwYxnDxccsX0",
        "outputId": "1beadf58-8f7e-49bb-d191-756960a35159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.695\n"
          ]
        }
      ],
      "source": [
        "count_results = []\n",
        "for i in range(len(review_scores)):\n",
        "    if review_scores[i] == real_scores[i]:\n",
        "        count_results.append(1)\n",
        "    else:\n",
        "        count_results.append(0)\n",
        "# print(count_results)\n",
        "\n",
        "count_accuracy = np.sum(count_results)/len(count_results)\n",
        "print(\"Accuracy:\" , count_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LibV4nR89BXb"
      },
      "source": [
        "# (2) Naive Bayes (9.5pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnF9adQnuwia"
      },
      "source": [
        "\n",
        "Your second task is to program a simple Machine Learning approach that operates\n",
        "on a simple Bag-of-Words (BoW) representation of the text data, as\n",
        "described by Pang et al. (2002). In this approach, the only features we\n",
        "will consider are the words in the text themselves, without bringing in\n",
        "external sources of information. The BoW model is a popular way of\n",
        "representing texts as vectors, making it\n",
        "easy to apply classical Machine Learning algorithms on NLP tasks.\n",
        "However, the BoW representation is also very crude, since it discards\n",
        "all information related to word order and grammatical structure in the\n",
        "original text—as the name suggests.\n",
        "\n",
        "## Writing your own classifier (4pts)\n",
        "\n",
        "Write your own code to implement the Naive Bayes (NB) classifier. As\n",
        "a reminder, the Naive Bayes classifier works according to the following\n",
        "equation:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} P(c|\\bar{f}) = \\operatorname*{arg\\,max}_{c \\in C} P(c)\\prod^n_{i=1} P(f_i|c)$$\n",
        "where $C = \\{ \\text{POS}, \\text{NEG} \\}$ is the set of possible classes,\n",
        "$\\hat{c} \\in C$ is the most probable class, and $\\bar{f}$ is the feature\n",
        "vector. Remember that we use the log of these probabilities when making\n",
        "a prediction:\n",
        "$$\\hat{c} = \\operatorname*{arg\\,max}_{c \\in C} \\Big\\{\\log P(c) + \\sum^n_{i=1} \\log P(f_i|c)\\Big\\}$$\n",
        "\n",
        "You can find more details about Naive Bayes in [Jurafsky &\n",
        "Martin](https://web.stanford.edu/~jurafsky/slp3/). You can also look at\n",
        "this helpful\n",
        "[pseudo-code](https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html).\n",
        "\n",
        "*Note: this section and the next aim to put you in a position to replicate\n",
        "    Pang et al.'s Naive Bayes results. However, your numerical results\n",
        "    will differ from theirs, as they used different data.*\n",
        "\n",
        "**You must write the Naive Bayes training and prediction code from\n",
        "scratch.** You will not be given credit for using off-the-shelf Machine\n",
        "Learning libraries.\n",
        "\n",
        "The data contains the text of the reviews, where each document consists\n",
        "of the sentences in the review, the sentiment of the review and an index\n",
        "(cv) that you will later use for cross-validation. The\n",
        "text has already been tokenised and POS-tagged for you. Your algorithm\n",
        "should read in the text, **lowercase it**, store the words and their\n",
        "frequencies in an appropriate data structure that allows for easy\n",
        "computation of the probabilities used in the Naive Bayes algorithm, and\n",
        "then make predictions for new instances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEpyQSBSkb33"
      },
      "source": [
        "#### (Q2.1) Unseen words (1pt)\n",
        "The presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive Bayes classifier to equal $0$.\n",
        "These can be words which are unseen in both positive and negative training reviews (case 1), but also words which are seen in reviews _of only one sentiment class_ in the training dataset (case 2). In both cases, **you should skip these words for both classes at test time**.  What would be the problem instead with skipping words only for one class in case 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BanFiYYnoxDW"
      },
      "source": [
        "\n",
        "When we skip words for only one class during test-time calculations, the classifier becomes biased. For example, if a word like \"xyz\" appears in the training set for class C1 but is absent from class C2, excluding this word for class C2 during testing effectively penalizes class C1. In Naive Bayes, probabilities are calculated by multiplying fractions, so removing a word from one class results in performing one less multiplication, leading to a larger overall probability for that class. This creates a bias in favor of class C2, regardless of how frequently \"xyz\" appears in C1. This approach is counterintuitive because observed words should increase the likelihood of a review belonging to their respective class, not reduce it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsZRhaI3WvzC"
      },
      "source": [
        "#### (Q2.2) Train your classifier on (positive and negative) reviews with cv-value 000-899, and test it on the remaining (positive and negative) reviews cv900–cv999.  Report results using classification accuracy as your evaluation metric. Your  features are the word vocabulary. The value of a feature is the count of that feature (word) in the document. (2pts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7zaJYGFvIJ3",
        "outputId": "d04d7767-5718-4640-8a5c-03d9b3367a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification accuracy: 0.825\n"
          ]
        }
      ],
      "source": [
        "# Split data into train and test sets\n",
        "train_reviews = [r for r in reviews if 0 <= r[\"cv\"] <= 899]\n",
        "test_reviews = [r for r in reviews if 900 <= r[\"cv\"] <= 999]\n",
        "\n",
        "# Vocabulary and word counts by class\n",
        "vocab = Counter()\n",
        "class_word_counts = {\"POS\": Counter(), \"NEG\": Counter()}\n",
        "class_counts = {\"POS\": 0, \"NEG\": 0}\n",
        "\n",
        "# Build vocabulary and class word counts from training data\n",
        "for review in train_reviews:\n",
        "    sentiment = review[\"sentiment\"]\n",
        "    class_counts[sentiment] += 1\n",
        "    for sentence in review[\"content\"]:\n",
        "        for token, pos_tag in sentence:\n",
        "            word = token.lower()\n",
        "            vocab[word] += 1\n",
        "            class_word_counts[sentiment][word] += 1\n",
        "\n",
        "def calculate_piors(train_reviews, class_counts):\n",
        "  # Calculate prior probabilities\n",
        "  total_train_reviews = len(train_reviews)\n",
        "  priors = {\n",
        "      \"POS\": class_counts[\"POS\"] / total_train_reviews,\n",
        "      \"NEG\": class_counts[\"NEG\"] / total_train_reviews\n",
        "  }\n",
        "\n",
        "  return priors\n",
        "\n",
        "# Function to calculate log-probabilities (skipping unseen words)\n",
        "def calculate_log_prob(review, sentiment, priors):\n",
        "    log_prob = math.log(priors[sentiment])\n",
        "    total_words_in_class = sum(class_word_counts[sentiment].values())\n",
        "\n",
        "    for sentence in review[\"content\"]:\n",
        "        for token, pos_tag in sentence:\n",
        "            word = token.lower()\n",
        "            # Ignore words unseen in the training set for either class\n",
        "            if word in class_word_counts[\"POS\"] and word in class_word_counts[\"NEG\"]:\n",
        "                word_count = class_word_counts[sentiment].get(word, 0)\n",
        "                log_prob += math.log(word_count / total_words_in_class)\n",
        "\n",
        "    return log_prob\n",
        "\n",
        "# Predict on test set and calculate accuracy\n",
        "correct_predictions = 0\n",
        "\n",
        "# Calculate prior probabilities\n",
        "priors = calculate_piors(train_reviews, class_counts)\n",
        "\n",
        "for review in test_reviews:\n",
        "    pos_log_prob = calculate_log_prob(review, \"POS\", priors)\n",
        "    neg_log_prob = calculate_log_prob(review, \"NEG\", priors)\n",
        "\n",
        "    predicted_sentiment = \"POS\" if pos_log_prob > neg_log_prob else \"NEG\"\n",
        "\n",
        "    if predicted_sentiment == review[\"sentiment\"]:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / len(test_reviews)\n",
        "print(\"Classification accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INK-PBoM6CB"
      },
      "source": [
        "#### (Q2.3) Would you consider accuracy to also be a good way to evaluate your classifier in a situation where 90% of your data instances are of positive movie reviews? (1pt)\n",
        "\n",
        "Simulate this scenario by keeping the positive reviews\n",
        "data unchanged, but only using negative reviews cv000–cv089 for\n",
        "training, and cv900–cv909 for testing. Calculate the classification\n",
        "accuracy, and explain what changed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFbcsYlipBAw"
      },
      "source": [
        "Using accuracy as an evaluation metric in scenarios with significant class imbalance is generally not advisable. In such cases, the classifier could easily achieve high accuracy by simply predicting the majority class for all instances. For example, if the test set is 90% positive, a classifier that always predicts the positive class would achieve 90% accuracy without actually learning anything meaningful about the data.\n",
        "\n",
        "However, this exact scenario may not apply to the Naive Bayes classifier The classifier will most likely be biased towards one of the classes, but not neccessarily towards the majority class. On one hand, the prior becomes much stronger for the majority class. On the other hand, the lower total word counts in the denominator will lead to higher likelihoods of the words in the minority class.\n",
        "\n",
        "In our case, we can see that the accuracy drops from 82.5% to 60% when using the imbalanced dataset. We can also notice that the classifier is biased towards the minority class by examining precision, recall and f1 scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL2Su5AoXOF9",
        "outputId": "2316e38b-4c58-472e-e72e-a0dead0f9afa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification accuracy: 0.6\n",
            "POS - Precision: 0.9828, Recall: 0.5700, F1 Score: 0.7215\n",
            "NEG - Precision: 0.1731, Recall: 0.9000, F1 Score: 0.2903\n",
            "Macro-averaged F1 Score: 0.5059\n"
          ]
        }
      ],
      "source": [
        "# Split data into train and test sets\n",
        "train_reviews_biased = [r for r in reviews if ((0 <= r[\"cv\"] <= 89) or ((90 <= r[\"cv\"] <= 899) and r['sentiment'] == 'POS'))]\n",
        "test_reviews_biased = [r for r in reviews if ((900 <= r[\"cv\"] <= 909) or ((900 <= r[\"cv\"] <= 999) and r['sentiment'] == 'POS'))]\n",
        "\n",
        "# Vocabulary and word counts by class\n",
        "vocab_biased = Counter()\n",
        "class_word_counts_biased = {\"POS\": Counter(), \"NEG\": Counter()}\n",
        "class_counts_biased = {\"POS\": 0, \"NEG\": 0}\n",
        "\n",
        "# Build vocabulary and class word counts from training data\n",
        "for review in train_reviews_biased:\n",
        "    sentiment = review[\"sentiment\"]\n",
        "    class_counts_biased[sentiment] += 1\n",
        "    for sentence in review[\"content\"]:\n",
        "        for token, pos_tag in sentence:\n",
        "            word = token.lower()\n",
        "            vocab_biased[word] += 1\n",
        "            class_word_counts_biased[sentiment][word] += 1\n",
        "\n",
        "# Calculate prior probabilities\n",
        "total_train_reviews_biased = len(train_reviews_biased)\n",
        "priors_biased = {\n",
        "    \"POS\": class_counts_biased[\"POS\"] / total_train_reviews_biased,\n",
        "    \"NEG\": class_counts_biased[\"NEG\"] / total_train_reviews_biased\n",
        "}\n",
        "\n",
        "# Function to calculate log-probabilities (skipping unseen words)\n",
        "def calculate_log_prob(review, sentiment):\n",
        "    log_prob = math.log(priors_biased[sentiment])\n",
        "    total_words_in_class = sum(class_word_counts_biased[sentiment].values())\n",
        "\n",
        "    for sentence in review[\"content\"]:\n",
        "        for token, pos_tag in sentence:\n",
        "            word = token.lower()\n",
        "            # Ignore words unseen in the training set for either class\n",
        "            if word in class_word_counts_biased[\"POS\"] and word in class_word_counts_biased[\"NEG\"]:\n",
        "                word_count = class_word_counts_biased[sentiment].get(word, 0)\n",
        "                log_prob += math.log(word_count / total_words_in_class)\n",
        "\n",
        "    return log_prob\n",
        "\n",
        "# Predict on test set and calculate accuracy, precision, recall, and F1 score\n",
        "correct_predictions = 0\n",
        "tp = {\"POS\": 0, \"NEG\": 0}  # True positives for each class\n",
        "fp = {\"POS\": 0, \"NEG\": 0}  # False positives for each class\n",
        "fn = {\"POS\": 0, \"NEG\": 0}  # False negatives for each class\n",
        "\n",
        "for review in test_reviews_biased:\n",
        "    pos_log_prob = calculate_log_prob(review, \"POS\")\n",
        "    neg_log_prob = calculate_log_prob(review, \"NEG\")\n",
        "\n",
        "    predicted_sentiment = \"POS\" if pos_log_prob > neg_log_prob else \"NEG\"\n",
        "    actual_sentiment = review[\"sentiment\"]\n",
        "\n",
        "    if predicted_sentiment == actual_sentiment:\n",
        "        correct_predictions += 1\n",
        "        tp[actual_sentiment] += 1\n",
        "    else:\n",
        "        fp[predicted_sentiment] += 1\n",
        "        fn[actual_sentiment] += 1\n",
        "\n",
        "accuracy = correct_predictions / len(test_reviews_biased)\n",
        "print(\"Classification accuracy:\", accuracy)\n",
        "\n",
        "# Calculate precision, recall, and F1 score for each class\n",
        "for sentiment in [\"POS\", \"NEG\"]:\n",
        "    precision = tp[sentiment] / (tp[sentiment] + fp[sentiment]) if (tp[sentiment] + fp[sentiment]) > 0 else 0\n",
        "    recall = tp[sentiment] / (tp[sentiment] + fn[sentiment]) if (tp[sentiment] + fn[sentiment]) > 0 else 0\n",
        "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"{sentiment} - Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1_score:.4f}\")\n",
        "\n",
        "# Calculate the macro-averaged F1 score\n",
        "macro_f1_score = sum((2 * (tp[s] / (tp[s] + fp[s])) * (tp[s] / (tp[s] + fn[s]))) / ((tp[s] / (tp[s] + fp[s])) + (tp[s] / (tp[s] + fn[s]))) if ((tp[s] / (tp[s] + fp[s])) + (tp[s] / (tp[s] + fn[s]))) > 0 else 0 for s in [\"POS\", \"NEG\"]) / 2\n",
        "print(f\"Macro-averaged F1 Score: {macro_f1_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bH79LXVhztx",
        "outputId": "217d69b9-b22e-4ea1-9b8f-23209bc1cab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'POS': 57, 'NEG': 9}\n",
            "{'POS': 1, 'NEG': 43}\n"
          ]
        }
      ],
      "source": [
        "print(tp)\n",
        "print(fp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnNukhJfccoG",
        "outputId": "cf398d9a-ccc9-41d2-b902-1cd712a38a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'POS': 0.9090909090909091, 'NEG': 0.09090909090909091}\n"
          ]
        }
      ],
      "source": [
        "print(priors_biased)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wJzcHX3WUDm"
      },
      "source": [
        "## Smoothing (1pt)\n",
        "\n",
        "As mentioned above, the presence of words in the test dataset that\n",
        "have not been seen during training can cause probabilities in the Naive\n",
        "Bayes classifier to be $0$, thus making that particular test instance\n",
        "undecidable. The standard way to mitigate this effect (as well as to\n",
        "give more clout to rare words) is to use smoothing, in which the\n",
        "probability fraction\n",
        "$$\\frac{\\text{count}(w_i, c)}{\\sum\\limits_{w\\in V} \\text{count}(w, c)}$$ for a word\n",
        "$w_i$ becomes\n",
        "$$\\frac{\\text{count}(w_i, c) + \\text{smoothing}(w_i)}{\\sum\\limits_{w\\in V} \\text{count}(w, c) + \\sum\\limits_{w \\in V} \\text{smoothing}(w)}$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBNIcbwUWphC"
      },
      "source": [
        "#### (Q2.4) Implement Laplace feature smoothing (1pt)\n",
        "Implement Laplace smoothing, i.e., smoothing with a constant value ($smoothing(w) = \\kappa, \\forall w \\in V$), in your Naive\n",
        "Bayes classifier’s code, and report the accuracy.\n",
        "Use $\\kappa = 1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g03yflCc9kpW",
        "outputId": "4293e9bf-9e83-43ca-a3ae-e55420f27cb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification accuracy: 0.825\n",
            "Classification accuracy on the biased dataset: 0.9181818181818182\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate log-probabilities (Laplace smoothing)\n",
        "def calculate_log_prob_Laplace(review, sentiment, priors, class_word_counts, vocab, stemmer = None):\n",
        "    log_prob = math.log(priors[sentiment])\n",
        "    total_words_in_class = sum(class_word_counts[sentiment].values())\n",
        "\n",
        "    for sentence in review[\"content\"]:\n",
        "        for token, pos_tag in sentence:\n",
        "            word = token.lower()\n",
        "            if stemmer is not None:\n",
        "                word = stemmer.stem(word)\n",
        "            # Laplace smoothing k = 1\n",
        "            word_count = class_word_counts[sentiment].get(word, 0) + 1\n",
        "            log_prob += math.log(word_count / (total_words_in_class + len(vocab)))\n",
        "\n",
        "    return log_prob\n",
        "\n",
        "def test_Laplace(test_reviews, priors, class_word_counts, vocab, stemmer = None):\n",
        "  # Predict on test set and calculate accuracy\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for review in test_reviews:\n",
        "      pos_log_prob = calculate_log_prob_Laplace(review, \"POS\", priors, class_word_counts, vocab, stemmer)\n",
        "      neg_log_prob = calculate_log_prob_Laplace(review, \"NEG\", priors, class_word_counts, vocab, stemmer)\n",
        "\n",
        "      predicted_sentiment = \"POS\" if pos_log_prob > neg_log_prob else \"NEG\"\n",
        "\n",
        "      if predicted_sentiment == review[\"sentiment\"]:\n",
        "          correct_predictions += 1\n",
        "\n",
        "  accuracy = correct_predictions / len(test_reviews)\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "# Report the accuracy\n",
        "print(\"Classification accuracy:\", test_Laplace(test_reviews, priors, class_word_counts, vocab))\n",
        "\n",
        "# Report the accuracy on biased dataset\n",
        "print(\"Classification accuracy on the biased dataset:\", test_Laplace(test_reviews_biased, priors_biased, class_word_counts_biased, vocab_biased))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aEoqjtnpQcI"
      },
      "source": [
        "Using Laplace smoothing on the balanced dataset did not affect classification accuracy, as both classes already had sufficient representation of words, making smoothing redundant.\n",
        "However, on the unbalanced dataset, Laplace smoothing led to a significant accuracy improvement, increasing from 60% to 91.82%. This improvement highlights the effectiveness of Laplace smoothing in handling unbalanced datasets. Unlike the approach of discarding unseen words, Laplace smoothing assigns a small, non-zero probability to all words, ensuring that rare or unseen words in the minority class still contribute to the classification.\n",
        "This correction eliminates the bias observed in Q2.3, where the model favored the minority class due to its lower total word count in the denominator. With Laplace smoothing, while the total word count is still lower for the minority class, it is counterbalanced by assigning low probabilities to unseen words, resulting in more balanced predictions and a significant boost in model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiGcgwba87D5"
      },
      "source": [
        "## Cross-Validation (1.5pts)\n",
        "\n",
        "A serious danger in using Machine Learning on small datasets, with many\n",
        "iterations of slightly different versions of the algorithms, is ending up with Type III errors, also called the “testing hypotheses\n",
        "suggested by the data” errors. This type of error occurs when we make\n",
        "repeated improvements to our classifiers by playing with features and\n",
        "their processing, but we don’t get a fresh, never-before seen test\n",
        "dataset every time. Thus, we risk developing a classifier that gets better\n",
        "and better on our data, but only gets worse at generalizing to new, unseen data. In other words, we risk developping a classifier that overfits.\n",
        "\n",
        "A simple method to guard against Type III errors is to use\n",
        "Cross-Validation. In **N-fold Cross-Validation**, we divide the data into N\n",
        "distinct chunks, or folds. Then, we repeat the experiment N times: each\n",
        "time holding out one of the folds for testing, training our classifier\n",
        "on the remaining N - 1 data folds, and reporting performance on the\n",
        "held-out fold. We can use different strategies for dividing the data:\n",
        "\n",
        "-   Consecutive splitting:\n",
        "  - cv000–cv099 = Split 1\n",
        "  - cv100–cv199 = Split 2\n",
        "  - etc.\n",
        "  \n",
        "-   Round-robin splitting (mod 10):\n",
        "  - cv000, cv010, cv020, … = Split 1\n",
        "  - cv001, cv011, cv021, … = Split 2\n",
        "  - etc.\n",
        "\n",
        "-   Random sampling/splitting\n",
        "  - Not used here (but you may choose to split this way in a non-educational situation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OeLcbSauGtR"
      },
      "source": [
        "#### (Q2.5) Write the code to implement 10-fold cross-validation using round-robin splitting for your Naive Bayes classifier from Q2.4 and compute the 10 accuracies. Report the final performance, which is the average of the performances per fold. If all splits perform equally well, this is a good sign. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMVhukbqykB9"
      },
      "outputs": [],
      "source": [
        "def build_vocab(train_reviews, stemmer = None):\n",
        "\n",
        "  # Vocabulary and word counts by class for training data\n",
        "  vocab = Counter()\n",
        "  class_word_counts = {\"POS\": Counter(), \"NEG\": Counter()}\n",
        "  class_counts = {\"POS\": 0, \"NEG\": 0}\n",
        "\n",
        "  # Build vocabulary and class word counts from training data\n",
        "  for review in train_reviews:\n",
        "      sentiment = review[\"sentiment\"]\n",
        "      class_counts[sentiment] += 1\n",
        "      for sentence in review[\"content\"]:\n",
        "          for token, pos_tag in sentence:\n",
        "              word = token.lower()\n",
        "              if stemmer is not None:\n",
        "                word = stemmer.stem(word)\n",
        "              vocab[word] += 1\n",
        "              class_word_counts[sentiment][word] += 1\n",
        "  return vocab, class_word_counts, class_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KeCGPa7Nuzx",
        "outputId": "a935cbc3-5d1a-459b-8a0e-96b10ddf1284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0 accuracy: 0.7900\n",
            "Fold 1 accuracy: 0.8350\n",
            "Fold 2 accuracy: 0.8100\n",
            "Fold 3 accuracy: 0.8300\n",
            "Fold 4 accuracy: 0.7750\n",
            "Fold 5 accuracy: 0.8500\n",
            "Fold 6 accuracy: 0.8300\n",
            "Fold 7 accuracy: 0.7800\n",
            "Fold 8 accuracy: 0.8250\n",
            "Fold 9 accuracy: 0.8450\n",
            "Average accuracy across 10 folds: 0.8170\n"
          ]
        }
      ],
      "source": [
        "# The function for 10-fold cross-validation with round-robin splitting\n",
        "def round_robin_cv(reviews, stemmer = None):\n",
        "    accuracies = []\n",
        "\n",
        "    # Loop over each fold from 0 to 9\n",
        "    for fold in range(10):\n",
        "        # Split the dataset into training and testing based on round-robin rule\n",
        "        train_reviews = [r for r in reviews if r[\"cv\"] % 10 != fold]\n",
        "        test_reviews = [r for r in reviews if r[\"cv\"] % 10 == fold]\n",
        "\n",
        "        vocab, class_word_counts, class_counts = build_vocab(train_reviews, stemmer)\n",
        "\n",
        "        # Calculate prior probabilities\n",
        "        priors = calculate_piors(train_reviews, class_counts)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        accuracy = test_Laplace(test_reviews, priors, class_word_counts, vocab, stemmer)\n",
        "        accuracies.append(accuracy)\n",
        "        print(f\"Fold {fold} accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Calculate and report final accuracy\n",
        "    average_accuracy = sum(accuracies) / len(accuracies)\n",
        "    print(f\"Average accuracy across 10 folds: {average_accuracy:.4f}\")\n",
        "    return average_accuracy, accuracies\n",
        "\n",
        "# Perform CV for the reviews dataset\n",
        "average_accuracy, accuracies = round_robin_cv(reviews)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq1ek_6mww6j"
      },
      "source": [
        "\n",
        "The average accuracy observed with cross-validation (81.70%) is comparable to the accuracy achieved without cross-validation in Q.2.4. However, it is slightly lower than the 82.5% accuracy reported in the non-cross-validated case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otdlsDXBNyOa"
      },
      "source": [
        "#### (Q2.6) Report the variance of the 10 accuracy scores. (0.5pt)\n",
        "\n",
        "**Please report all future results using 10-fold cross-validation now\n",
        "(unless told to use the held-out test set).** Note: you're not allowed to use a library for computing the variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoBQm1KuNzNR",
        "outputId": "11b4559c-65e4-4132-c8dd-102abab0193d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Variance of the 10 accuracy scores: 0.0006509999999999987\n"
          ]
        }
      ],
      "source": [
        "# Calculate variance using previously found average accuracy and list of all accuracies\n",
        "variance = sum((accuracy - average_accuracy) ** 2 for accuracy in accuracies) / len(accuracies)\n",
        "\n",
        "print(\"Variance of the 10 accuracy scores:\", variance)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6A2zX9_BRKm"
      },
      "source": [
        "## Features, overfitting, and the curse of dimensionality\n",
        "\n",
        "In the Bag-of-Words model, ideally we would like each distinct word in\n",
        "the text to be mapped to its own dimension in the output vector\n",
        "representation. However, real world text is messy, and we need to decide\n",
        "on what we consider to be a word. For example, is “`word`\" different\n",
        "from “`Word`\", from “`word`”, or from “`words`\"? Too strict a\n",
        "definition, and the number of features explodes, while our algorithm\n",
        "fails to learn anything generalisable. Too lax, and we risk destroying\n",
        "our learning signal. In the following section, you will learn about\n",
        "confronting the feature sparsity and the overfitting problems as they\n",
        "occur in NLP classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKK8FNt8VtcZ"
      },
      "source": [
        "### Stemming (1.5pts)\n",
        "\n",
        "To make your algorithm more robust, use stemming and hash different inflections of a word to the same feature in the BoW vector space. Please use the [Porter stemming\n",
        "    algorithm](http://www.nltk.org/howto/stem.html) from NLTK.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxtCul1IrBi_",
        "outputId": "0c8fb263-3a44-42e4-8490-8bccaf5d790a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Already included the possibility of using the stemmer in previously defined helper functions.\n"
          ]
        }
      ],
      "source": [
        "print('Already included the possibility of using the stemmer in previously defined helper functions.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SrJ1BeLXTnk"
      },
      "source": [
        "#### (Q2.7): How does the performance of your classifier change when you use stemming on your training and test datasets? (1pt)\n",
        "Use cross-validation to evaluate the classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYqKBOiIrInT",
        "outputId": "d305f6c8-2dd4-4b00-f738-841a5650b7f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 0 accuracy: 0.7800\n",
            "Fold 1 accuracy: 0.8400\n",
            "Fold 2 accuracy: 0.8100\n",
            "Fold 3 accuracy: 0.8500\n",
            "Fold 4 accuracy: 0.7750\n",
            "Fold 5 accuracy: 0.8350\n",
            "Fold 6 accuracy: 0.8100\n",
            "Fold 7 accuracy: 0.7750\n",
            "Fold 8 accuracy: 0.8300\n",
            "Fold 9 accuracy: 0.8400\n",
            "Average accuracy across 10 folds: 0.8145\n"
          ]
        }
      ],
      "source": [
        "# Perform CV for the reviews dataset using Porter stemmer\n",
        "average_accuracy, accuracies = round_robin_cv(reviews, stemmer = PorterStemmer())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWovxWThMmkY"
      },
      "source": [
        "The use of stemming results in a slight decrease in average accuracy, dropping from 81.70% in the base case to 81.45% when stemming is applied."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkDHVq_1XUVP"
      },
      "source": [
        "#### (Q2.8) What happens to the number of features (i.e., the size of the vocabulary) when using stemming as opposed to (Q2.4)? (0.5pt)\n",
        "Give actual numbers. You can use the held-out training set to determine these."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-aqxajPz_Pk"
      },
      "source": [
        "Not sure what was meant by 'held-out training set'. Performing all calculations on 'train_reviews' created in Q2.2 and reused in Q2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA3vee5-rJyy",
        "outputId": "2fbf6912-23b8-49d8-f82f-a03312f8418c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of training samples: 1800\n",
            "Size of the vocabulary when not using stemming: 45348\n",
            "Size of the vocabulary when using Porter stemming: 32404\n"
          ]
        }
      ],
      "source": [
        "print(f'Amount of training samples: {len(train_reviews)}')\n",
        "\n",
        "vocab_no_stemmer,_,_ = build_vocab(train_reviews, stemmer = None)\n",
        "print(f'Size of the vocabulary when not using stemming: {len(vocab_no_stemmer)}')\n",
        "\n",
        "vocab_porter,_,_ = build_vocab(train_reviews, stemmer = PorterStemmer())\n",
        "print(f'Size of the vocabulary when using Porter stemming: {len(vocab_porter)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAywRohq4g-q"
      },
      "source": [
        "The vocabulary size decreases significantly when stemming is applied, which is an expected outcome. Stemming works by reducing related words to a common root form, thereby consolidating multiple word forms into a single stem. For instance, words like 'relaxed' and 'relaxing' are both reduced to the stem 'relax', effectively shrinking the vocabulary size by eliminating variations of the same word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoazfxbNV5Lq"
      },
      "source": [
        "### N-grams (1.5pts)\n",
        "\n",
        "A simple way of retaining some of the word\n",
        "order information when using bag-of-words representations is to use **n-gram** features.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHjy3I7-qWiu"
      },
      "source": [
        "#### (Q2.9) Retrain your classifier from (Q2.4) using **unigrams+bigrams** and **unigrams+bigrams+trigrams** as features. (1pt)\n",
        "Report accuracy and compare it with that of the approaches you have previously implemented. You are allowed to use NLTK to build n-grams from sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrN3BMTh9PNs"
      },
      "outputs": [],
      "source": [
        "# Define function to create n-grams from sentences\n",
        "def create_ngrams(tokens, n):\n",
        "    return [' '.join(gram) for gram in ngrams(tokens, n)]\n",
        "\n",
        "def build_vocab_ngram(train_reviews, stemmer = None, ngram_values = [1]):\n",
        "\n",
        "  # Vocabulary and word counts by class for training data\n",
        "  vocab = Counter()\n",
        "  class_word_counts = {\"POS\": Counter(), \"NEG\": Counter()}\n",
        "  class_counts = {\"POS\": 0, \"NEG\": 0}\n",
        "\n",
        "  # Build vocabulary and class word counts from training data\n",
        "  for review in train_reviews:\n",
        "      sentiment = review[\"sentiment\"]\n",
        "      class_counts[sentiment] += 1\n",
        "      for sentence in review[\"content\"]:\n",
        "          # Tokenize the sentence\n",
        "          tokens = [token.lower() for token, pos_tag in sentence]\n",
        "          if stemmer is not None:\n",
        "              tokens = [stemmer.stem(token) for token in tokens]\n",
        "          # Create n-grams based on the specified values\n",
        "          for n in ngram_values:\n",
        "              n_grams = create_ngrams(tokens, n)\n",
        "              for n_gram in n_grams:\n",
        "                  vocab[n_gram] += 1\n",
        "                  class_word_counts[sentiment][n_gram] += 1\n",
        "\n",
        "  return vocab, class_word_counts, class_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYj-m17KQw7M",
        "outputId": "46827cb4-caee-4663-d117-b70eedfa0877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using unigrams and bigrams\n",
            "Fold 0 accuracy: 0.8000\n",
            "Fold 1 accuracy: 0.8650\n",
            "Fold 2 accuracy: 0.8350\n",
            "Fold 3 accuracy: 0.8700\n",
            "Fold 4 accuracy: 0.8150\n",
            "Fold 5 accuracy: 0.8750\n",
            "Fold 6 accuracy: 0.8250\n",
            "Fold 7 accuracy: 0.8400\n",
            "Fold 8 accuracy: 0.8650\n",
            "Fold 9 accuracy: 0.8450\n",
            "Average accuracy across 10 folds: 0.8435\n",
            "\n",
            "\n",
            "------------------------------\n",
            "\n",
            "\n",
            "Using unigrams, bigrams and trigrams\n",
            "Fold 0 accuracy: 0.8000\n",
            "Fold 1 accuracy: 0.8700\n",
            "Fold 2 accuracy: 0.8500\n",
            "Fold 3 accuracy: 0.8800\n",
            "Fold 4 accuracy: 0.8100\n",
            "Fold 5 accuracy: 0.9000\n",
            "Fold 6 accuracy: 0.8250\n",
            "Fold 7 accuracy: 0.8400\n",
            "Fold 8 accuracy: 0.8700\n",
            "Fold 9 accuracy: 0.8350\n",
            "Average accuracy across 10 folds: 0.8480\n"
          ]
        }
      ],
      "source": [
        "# Define function for 10-fold cross-validation with round-robin splitting\n",
        "def round_robin_cv_ngram(reviews, stemmer=None, ngram_values=[1]):\n",
        "    accuracies = []\n",
        "\n",
        "    for fold in range(10):\n",
        "        train_reviews = [r for r in reviews if r[\"cv\"] % 10 != fold]\n",
        "        test_reviews = [r for r in reviews if r[\"cv\"] % 10 == fold]\n",
        "\n",
        "        # Use the new function to build the vocabulary and class counts\n",
        "        vocab, class_word_counts, class_counts = build_vocab_ngram(train_reviews, stemmer, ngram_values)\n",
        "\n",
        "        # Calculate prior probabilities\n",
        "        total_train_reviews = len(train_reviews)\n",
        "        priors = {\n",
        "            \"POS\": class_counts[\"POS\"] / total_train_reviews,\n",
        "            \"NEG\": class_counts[\"NEG\"] / total_train_reviews\n",
        "        }\n",
        "\n",
        "        # Function to calculate log-probabilities with Laplace smoothing\n",
        "        def calculate_log_prob(review, sentiment):\n",
        "            log_prob = math.log(priors[sentiment])\n",
        "            total_words_in_class = sum(class_word_counts[sentiment].values())\n",
        "\n",
        "            for sentence in review[\"content\"]:\n",
        "                tokens = [token.lower() for token, pos_tag in sentence]\n",
        "                for n in ngram_values:\n",
        "                    n_grams = create_ngrams(tokens, n)\n",
        "                    for n_gram in n_grams:\n",
        "                        word_count = class_word_counts[sentiment].get(n_gram, 0) + 1\n",
        "                        log_prob += math.log(word_count / (total_words_in_class + len(vocab)))\n",
        "\n",
        "            return log_prob\n",
        "\n",
        "        # Predict on test set and calculate accuracy\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for review in test_reviews:\n",
        "            pos_log_prob = calculate_log_prob(review, \"POS\")\n",
        "            neg_log_prob = calculate_log_prob(review, \"NEG\")\n",
        "\n",
        "            predicted_sentiment = \"POS\" if pos_log_prob > neg_log_prob else \"NEG\"\n",
        "            if predicted_sentiment == review[\"sentiment\"]:\n",
        "                correct_predictions += 1\n",
        "\n",
        "        accuracy = correct_predictions / len(test_reviews)\n",
        "        accuracies.append(accuracy)\n",
        "        print(f\"Fold {fold} accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    final_performance = sum(accuracies) / len(accuracies)\n",
        "    print(f\"Average accuracy across 10 folds: {final_performance:.4f}\")\n",
        "\n",
        "# Perform CV for the reviews dataset using 1,2-grams\n",
        "print('Using unigrams and bigrams')\n",
        "round_robin_cv_ngram(reviews, ngram_values=[1,2])\n",
        "\n",
        "# Perform CV for the reviews dataset using 1,2,3-grams\n",
        "print('\\n')\n",
        "print('-'*30)\n",
        "print('\\n')\n",
        "print('Using unigrams, bigrams and trigrams')\n",
        "round_robin_cv_ngram(reviews, ngram_values=[1,2,3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0qJQDGRycut"
      },
      "source": [
        "\n",
        "Incorporating both unigrams and bigrams leads to a noticeable improvement in performance, with the average accuracy increasing to 84.35%. This outperforms the cases where only unigrams (81.70%) or stemmed unigrams (81.45%) were used. Adding trigrams provides a slight additional boost, bringing the average accuracy to 84.40%. This demonstrates that using a combination of n-grams can capture more contextual information, leading to better overall accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVrGGArkrWoL"
      },
      "source": [
        "\n",
        "#### Q2.10: How many features does the BoW model have to take into account now? (0.5pt)\n",
        "How would you expect the number of features to increase theoretically (e.g., linear, square, cubed, exponential)? How do the number of features increase in the held-out training set (compared to Q2.8)?\n",
        "Do you expect this rate of increase to continue for (much) larger n-grams?\n",
        "\n",
        "Use the held-out training set once again for this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v20fhlhr1HOF"
      },
      "source": [
        "Same as in Q2.8. Not sure what was meant by 'held-out training set'.\n",
        "\n",
        "Performing all calculations on 'train_reviews' created in Q2.2 and reused in Q2.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEGZ9SV8pPaa"
      },
      "source": [
        "The number of features could theoretically grow exponentially as we include higher-order n-grams. However, in practice, most potential n-grams do not appear in the observed sentences, so the actual growth is much slower than the theoretical maximum.\n",
        "Based on calculations performed on the 10th cross-validation (CV) training set:\n",
        "- The size of the unigram vocabulary is 45,348.\n",
        "- The size of the unigram + bigram vocabulary is 465,262, which is approximately 10 times the size of the unigram vocabulary.\n",
        "- The size of the unigram + bigram + trigram vocabulary is 1,346,107, which is approximately:\n",
        " - 30 times the size of the unigram vocabulary.\n",
        " - 3 times the size of the unigram + bigram vocabulary.\n",
        "\n",
        "These figures show that while the feature set grows significantly when moving from unigrams to bigrams, the rate of growth diminishes when adding trigrams. This trend suggests that as we incorporate larger n-grams, the rate of increase in vocabulary size will continue to slow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z8sAJeUrdtM",
        "outputId": "786f985e-3e79-40aa-ca59-ec855e1a8d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Amount of training samples: 1800\n",
            "Size of the unigram vocabulary when not using stemming: 45348\n",
            "Size of the unigram vocabulary when using Porter stemming: 32404\n",
            "Size of the 1,2-gram vocabulary when not using stemming: 465262\n",
            "Size of the 1,2-gram vocabulary when using Porter stemming: 407943\n",
            "Size of the 1,2,3-gram vocabulary when not using stemming: 1346107\n",
            "Size of the 1,2,3-gram vocabulary when using Porter stemming: 1265912\n"
          ]
        }
      ],
      "source": [
        "print(f'Amount of training samples: {len(train_reviews)}')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "vocab_no_stemmer,_,_ = build_vocab(train_reviews, stemmer = None)\n",
        "print(f'Size of the unigram vocabulary when not using stemming: {len(vocab_no_stemmer)}')\n",
        "\n",
        "vocab_porter,_,_ = build_vocab(train_reviews, stemmer)\n",
        "print(f'Size of the unigram vocabulary when using Porter stemming: {len(vocab_porter)}')\n",
        "\n",
        "two_gram_vocab_no_stemmer,_,_ = build_vocab_ngram(train_reviews, stemmer = None, ngram_values = [1,2])\n",
        "print(f'Size of the 1,2-gram vocabulary when not using stemming: {len(two_gram_vocab_no_stemmer)}')\n",
        "\n",
        "two_gram_vocab_porter,_,_ = build_vocab_ngram(train_reviews, stemmer, ngram_values = [1,2])\n",
        "print(f'Size of the 1,2-gram vocabulary when using Porter stemming: {len(two_gram_vocab_porter)}')\n",
        "\n",
        "three_gram_vocab_no_stemmer,_,_ = build_vocab_ngram(train_reviews, stemmer = None, ngram_values = [1,2,3])\n",
        "print(f'Size of the 1,2,3-gram vocabulary when not using stemming: {len(three_gram_vocab_no_stemmer)}')\n",
        "\n",
        "three_gram_vocab_porter,_,_ = build_vocab_ngram(train_reviews, stemmer, ngram_values = [1,2,3])\n",
        "print(f'Size of the 1,2,3-gram vocabulary when using Porter stemming: {len(three_gram_vocab_porter)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "EK2n94uOGRWn",
        "outputId": "fe62fb93-a521-4f93-a842-a3c157f14a44"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Collecting the vocabulary sizes\n",
        "sizes = {\n",
        "    'Unigram': [len(vocab_no_stemmer), len(vocab_porter)],\n",
        "    '1,2-Gram': [len(two_gram_vocab_no_stemmer), len(two_gram_vocab_porter)],\n",
        "    '1,2,3-Gram': [len(three_gram_vocab_no_stemmer), len(three_gram_vocab_porter)],\n",
        "}\n",
        "\n",
        "# Setting the x-axis labels and bar width\n",
        "labels = list(sizes.keys())\n",
        "bar_width = 0.35\n",
        "\n",
        "# Create the figure and axis\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Setting positions for the bars\n",
        "x = range(len(labels))\n",
        "\n",
        "# Plotting the bars for each set of vocabulary sizes\n",
        "bars1 = ax.bar([i - bar_width/2 for i in x], [sizes[label][0] for label in labels],\n",
        "                width=bar_width, label='Without Stemming', color='lightblue')\n",
        "bars2 = ax.bar([i + bar_width/2 for i in x], [sizes[label][1] for label in labels],\n",
        "                width=bar_width, label='With Porter Stemming', color='salmon')\n",
        "\n",
        "# Adding labels and title\n",
        "ax.set_xlabel('N-Gram Type')\n",
        "ax.set_ylabel('Vocabulary Size')\n",
        "ax.set_title('Vocabulary Sizes with and without Stemming')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHWKDL3YV6vh"
      },
      "source": [
        "# (3) Support Vector Machines (4pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJSYhcVaoJGt"
      },
      "source": [
        "Though simple to understand, implement, and debug, one\n",
        "major problem with the Naive Bayes classifier is that its performance\n",
        "deteriorates (becomes skewed) when it is being used with features which\n",
        "are not independent (i.e., are correlated). Another popular classifier\n",
        "that doesn’t scale as well to big data, and is not as simple to debug as\n",
        "Naive Bayes, but that doesn’t assume feature independence is the Support\n",
        "Vector Machine (SVM) classifier.\n",
        "\n",
        "You can find more details about SVMs in Chapter 7 of Bishop: Pattern Recognition and Machine Learning.\n",
        "Other sources for learning SVM:\n",
        "* http://web.mit.edu/zoya/www/SVM.pdf\n",
        "* http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf\n",
        "* https://pythonprogramming.net/support-vector-machine-intro-machine-learning-tutorial/\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Use the scikit-learn implementation of\n",
        "[SVM](http://scikit-learn.org/stable/modules/svm.html) with the default parameters. (You are not expected to perform any hyperparameter tuning, but feel free to do it if you think it gives you good insights for the discussion in question 5.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LnzNtQBV8gr"
      },
      "source": [
        "#### (Q3.1): Train SVM and compare to Naive Bayes (2pts)\n",
        "\n",
        "Train an SVM classifier (sklearn.svm.LinearSVC) using the features collected for Naive Bayes. Compare the\n",
        "classification performance of the SVM classifier to that of the Naive\n",
        "Bayes classifier with smoothing.\n",
        "Use cross-validation to evaluate the performance of the classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBscui8Mvoz0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "X = [\" \".join([token for sentence in review[\"content\"] for token, pos_tag in sentence]) for review in reviews]\n",
        "y = [1 if review[\"sentiment\"] == \"POS\" else 0 for review in reviews]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_vec = vectorizer.fit_transform(X)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHr_-6LhcsY1",
        "outputId": "470d633c-78dc-42b4-e2fa-fde58a11dd36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Performance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.82       199\n",
            "           1       0.83      0.82      0.82       201\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.82      0.82      0.82       400\n",
            "weighted avg       0.82      0.82      0.82       400\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_clf = LinearSVC(max_iter=10000)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm = svm_clf.predict(X_test)\n",
        "\n",
        "print(\"Performance:\")\n",
        "print(classification_report(y_test, y_pred_svm))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "RdsXeZ7ncsY5",
        "outputId": "f7da842c-c3ef-45c9-ae49-1484a0291ca8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross-Validation Accuracy: 0.8425 +/- 0.0211\n"
          ]
        }
      ],
      "source": [
        "svm_cv_scores = cross_val_score(svm_clf, X_vec, y, cv=10, scoring='accuracy')\n",
        "print(f\"Cross-Validation Accuracy: {svm_cv_scores.mean():.4f} +/- {svm_cv_scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9el9stucsY7"
      },
      "source": [
        "> Looking at the Naive Bayes implementaiton in Section 2, we can see the Naive Bayes with smoothing achieved an accuracy of 82.5%, while the SVM achieves accuracy of between 81-86%. Looking at cross-validation provides the differnce in their accuracy rates within trianing , as the Naive Bayes cross-validation score (taking the avarage accuracy over 10 folds) was up to 95.95% while the SVM cross-validation across ten folds was only 84.25%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifXVWcK0V9qY"
      },
      "source": [
        "### POS disambiguation (2pts)\n",
        "\n",
        "Now add in part-of-speech features. You will find the\n",
        "movie review dataset has already been POS-tagged for you ([here](https://catalog.ldc.upenn.edu/docs/LDC99T42/tagguid1.pdf) you find the tagset). Try to\n",
        "replicate the results obtained by Pang et al. (2002).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xA3I82o4oWGu"
      },
      "source": [
        "####(Q3.2) Replace your features with word+POS features, and report performance with the SVM. Use cross-validation to evaluate the classifier and compare the results with (Q3.1). Does part-of-speech information help? Explain why this may be the case. (1pt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOvjYe-t2Br6",
        "outputId": "12460e4b-f447-435d-8924-f79d89c77aba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Classifier Performance (Word+POS tag) Test Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.83      0.82       199\n",
            "           1       0.83      0.82      0.82       201\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.82      0.82      0.82       400\n",
            "weighted avg       0.82      0.82      0.82       400\n",
            "\n",
            "SVM Classifier Cross-Validation Accuracy (Word+POS tags): 0.8460 +/- 0.0166\n"
          ]
        }
      ],
      "source": [
        "X_pos = [\" \".join([f\"{token}_{pos_tag}\" for sentence in review[\"content\"] for token, pos_tag in sentence]) for review in reviews]\n",
        "y = [1 if review[\"sentiment\"] == \"POS\" else 0 for review in reviews]\n",
        "\n",
        "\n",
        "# print(X_pos[0:5])\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_vec_pos = vectorizer.fit_transform(X_pos)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec_pos, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_clf_pos = LinearSVC(max_iter=100000)\n",
        "svm_clf_pos.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm_pos = svm_clf_pos.predict(X_test)\n",
        "\n",
        "print(\"SVM Classifier Performance (Word+POS tag) Test Set:\")\n",
        "print(classification_report(y_test, y_pred_svm_pos))\n",
        "\n",
        "svm_cv_scores_pos = cross_val_score(svm_clf_pos, X_vec_pos, y, cv=10, scoring='accuracy')\n",
        "print(f\"SVM Classifier Cross-Validation Accuracy (Word+POS tags): {svm_cv_scores_pos.mean():.4f} +/- {svm_cv_scores_pos.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0dt_oQupUNe"
      },
      "source": [
        "> There is a slight improvement looking a the Cross-Validation accuracy, which with POS features is 84.60% while without POS it was 84.25%. While small, there is an improvement using POS tagging information. This is likely because it potentiall helps add context so that the SVM can better understand the significance of the sentence structure and syntactic roles, such as highlighting the importance of adjectives in determining the sentiment. From a theoretical perspective including this information then makes sense given its inherent additinal gramatical information which can highlight nuances in language previously unseen, and perhaps a more in depth implementation of the token-POS tag relation might yield even better results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su-3w87eMW0w"
      },
      "source": [
        "#### (Q3.3) Discard all closed-class words from your data (keep only nouns, verbs, adjectives, and adverbs), and report performance. Does this help? Use cross-validation to evaluate the classifier and compare the results with (Q3.2). Are closed-class words detrimental to the classifier? Explain why this may be the case. (1pt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCUPlPozCYUX",
        "outputId": "24025c15-5b6e-4f12-8e25-015c51d6c716"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM Classifier Performance (Open-Class Features Only) Test Set:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82       199\n",
            "           1       0.83      0.81      0.82       201\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.82      0.82      0.82       400\n",
            "weighted avg       0.82      0.82      0.82       400\n",
            "\n",
            "SVM Classifier Cross-Validation Accuracy (Open-Class Features Only): 0.8445 +/- 0.0223\n"
          ]
        }
      ],
      "source": [
        "open_class_tags = {'NN', 'NNS', 'NNP', 'NNPS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS'}\n",
        "X_open_class = [\" \".join([f\"{token}\" for sentence in review[\"content\"]\n",
        "                         for token, pos_tag in sentence if pos_tag in open_class_tags]) for review in reviews]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_vec_open_class = vectorizer.fit_transform(X_open_class)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec_open_class, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_clf_open_class = LinearSVC(max_iter=10000)\n",
        "svm_clf_open_class.fit(X_train, y_train)\n",
        "\n",
        "y_pred_svm_open_class = svm_clf_open_class.predict(X_test)\n",
        "print(\"SVM Classifier Performance (Open-Class Features Only) Test Set:\")\n",
        "print(classification_report(y_test, y_pred_svm_open_class))\n",
        "\n",
        "svm_cv_scores_open_class = cross_val_score(svm_clf_open_class, X_vec_open_class, y, cv=10, scoring='accuracy')\n",
        "print(f\"SVM Classifier Cross-Validation Accuracy (Open-Class Features Only): {svm_cv_scores_open_class.mean():.4f} +/- {svm_cv_scores_open_class.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Km-fgE6JcsZI"
      },
      "source": [
        "> Getting rid of all closed-form words improved the performace. We are using a model similar to 3.1. Accuracy is also 82% while using the same seed as for the SVM with all words inluded. The cross-validation accuracy for this SVM is 84.45%, again an improvement as compared to SVM in 3.1 which was 84.25%. While only slight, it is much more efficient than the implementation in 3.2 considering the vocabulary size. While I wouldn't go as far as to say these words are detrimental, as even with them we get a reasonable accuracy, it makes the model more efficient to train without them which makes it a worthwhile endavor for future applications on larger datasets. In theory I cannot think of a reason to keep them especially without using context or surrounding words such as here, which should result in a more robust classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfwqOciAl2No"
      },
      "source": [
        "# (4) Discussion (max. 500 words). (5pts)\n",
        "\n",
        "> Based on your experiments, what are the effective features and techniques in sentiment analysis? What information do different features encode?\n",
        "Why is this important? What are the limitations of these features and techniques?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYuse5WLmekZ"
      },
      "source": [
        "*Write your answer here in up to 500 words (-0.25pt for >50 extra words, -0.5 points for >100 extra words, ...)*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwaKwfWQhRk_"
      },
      "source": [
        "# Submission\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOUeaET5ijk-"
      },
      "outputs": [],
      "source": [
        "# Write your names and student numbers here:\n",
        "# Piotr Sobecki # 15127206\n",
        "# Lucia Sikulova # 13549634"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A9K-H6Tii3X"
      },
      "source": [
        "**That's it!**\n",
        "\n",
        "- Check if you answered all questions fully and correctly.\n",
        "- Download your completed notebook using `File -> Download .ipynb`\n",
        "- Check if your answers are all included in the file you submit.\n",
        "- Submit your .ipynb file via *Canvas*. One submission per group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHslatYAKBrF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}